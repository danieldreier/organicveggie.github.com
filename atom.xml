<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Blind Squirrel]]></title>
  <link href="http://organicveggie.github.com/atom.xml" rel="self"/>
  <link href="http://organicveggie.github.com/"/>
  <updated>2013-03-12T13:43:16-07:00</updated>
  <id>http://organicveggie.github.com/</id>
  <author>
    <name><![CDATA[Sean Laurent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Distributed Graphite on EC2]]></title>
    <link href="http://organicveggie.github.com/blog/2012/05/28/distributed-graphite-on-ec2/"/>
    <updated>2012-05-28T14:20:00-07:00</updated>
    <id>http://organicveggie.github.com/blog/2012/05/28/distributed-graphite-on-ec2</id>
    <content type="html"><![CDATA[<p>Some time ago, we switched from <a href="http://www.cacti.net/">Cacti</a> to <a href="http://graphite.wikidot.com/">Graphite</a> for tracking and graphing system metrics. In our <a href="http://aws.amazon.com/ec2/">Amazon EC2 environment</a>, we frequently startup new servers and shut down old servers. While Cacti did a decent job, the amount of manual effort required to setup new graphs made it a challenge to use. Since Graphite simply tracks anything you throw at it and easily handles applying aggregate functions across multiple metrics, we found Graphite to be a much better fit with EC2 and our usage patterns. With a solid web interface and a variety of alternate front-ends and awesome dashboard tools like <a href="http://jondot.github.com/graphene/">Graphene</a>, we quickly fell in love with Graphite.</p>

<!-- more -->


<p>In fact, the more we utilized Graphite, the more data we wanted to push into Graphite. Systems metrics? Check. IO stats? Check. Apache stats? Check. MongoDB stats on a per collection basis? Why not! We quickly found ourselves with over 50,000 metrics and 500GB of data, all of which was getting updated every 15-30 seconds. Unfortunately, we also rapidly hit the limit of how much data we could record on a single instance. Even with a RAID-0 array of 8x EBS volumes on an m2.2xlarge instance, we couldn&#8217;t write the data fast enough. The poor instance began spending more than 50% of it&#8217;s time in I/O wait  and eventually became non-responsive. What we needed was a way to distribute the write load across multiple instances.</p>

<p>Fortunately, Graphite natively supports running in a distributed environment. A successful deployment consists of three key pieces:</p>

<ul>
<li>carbon-cache - This is what actually accepts and stores the metrics.</li>
<li>carbon-relay - Handles sharding the metrics and sending them to the appropriate carbon-cache instances.</li>
<li>webapp - Handles displaying the metrics. Must reside on every instance running a carbon-cache daemon. The webapp queries the other carbon-cache instances for data as necessary.</li>
</ul>


<p>Although there are many different ways you can setup your distributed Graphite environment, we chose to take a straight foward approach:</p>

<ul>
<li>One dedicated instance running carbon-relay that receives metrics from the various server instances, shards the metrics and forwards them on to the carbon-cache servers</li>
<li>Two dedicated instances running both carbon-cache and webapp.</li>
</ul>


<h2>Carbon Relay</h2>

<p>On the carbon-relay server, you need to configure two key files to indicate that this node should run as a relay: carbon.conf and relay.conf.</p>

<p><code>carbon.conf</code></p>

<div>
  <pre><code class='properties'>[relay]
RELAY_METHOD = consistent-hashing

LINE_RECEIVER_INTERFACE = x.x.x.x
LINE_RECEIVER_PORT = 2003

PICKLE_RECEIVER_INTERFACE = 127.0.0.1
PICKLE_RECEIVER_PORT = 2004

DESTINATIONS = cache1:2004:a,cache2:2004:a

MAX_DATAPOINTS_PER_MESSAGE = 500
MAX_QUEUE_SIZE = 10000

USE_FLOW_CONTROL = True</code></pre>
</div>


<p>For simplicity, we&#8217;re using the consistent-hashing method of sharding. The advantage to consistent-hasing is that Graphite will automatically split the metrics evenly across all carbon-cache instances. If you add another carbon-cache node, Graphite will start sending data to it. The disadvantage is that you have absolutely no control over which metric goes to which server. If you start with two nodes and then add a third node, Graphite will not automatically rebalance the data.</p>

<p>A couple key things to note:</p>

<ul>
<li>The RELAY_METHOD value is set to consistent-hashing</li>
<li>The LINE_RECEIVER_INTERFACE must be set to the IP address of the relay node, not localhost</li>
<li>The DESTINATIONS value must contain all of the target carbon-cache nodes</li>
<li>Each server listed in the DESTINATIONS option must include the name or ip address of a carbon-cache instance, the pickle-receiver port for the remote instance and the carbon-cache identifier. If you run multiple carbon-cache daemons on the remote server,  the first instance will be identified as &#8220;a&#8221;, the second instance as &#8220;b&#8221;, etc.</li>
</ul>


<p><code>relay.conf</code></p>

<div>
  <pre><code class='properties'>[default]
default = true
destinations = cache1,cache2</code></pre>
</div>


<p>This sample relay.conf shows us splitting the data automatically across two servers: cache1 and cache2.</p>

<h2>Carbon Cache</h2>

<p>On the carbon-cache servers, we need to configure the carbon-cache daemon to appropriately accept incoming data from the carbon-relay servers.</p>

<p><code>carbon.conf</code></p>

<div>
  <pre><code class='properties'>[cache]
LOCAL_DATA_DIR = /opt/graphite/storage/whisper/

MAX_CACHE_SIZE = inf

MAX_UPDATES_PER_SECOND = 800

MAX_CREATES_PER_MINUTE = 100

LINE_RECEIVER_INTERFACE = y.y.y.y
LINE_RECEIVER_PORT = 2003

PICKLE_RECEIVER_INTERFACE = y.y.y.y
PICKLE_RECEIVER_PORT = 2004

CACHE_QUERY_INTERFACE = 127.0.0.1
CACHE_QUERY_PORT = 7002

LOG_UPDATES = False</code></pre>
</div>


<p>There are several critical settings here. First, the pickle receiver must be configured correctly to allow incoming data from the carbon-relay server. Second, the line receiver must be configured correctly to allow the the webapp to communicate with the other carbon-cache instances.</p>

<ul>
<li>LINE_RECEIVER_INTERFACE must be set to the local ip address (not 127.0.0.1)</li>
<li>LINE_RECEIVER_PORT must be set to 2003</li>
<li>PICKLE_RECEIVER_INTERFACE must be set to the local ip address (not 127.0.0.1)</li>
<li>PICKLE_RECEIVER_PORT must match the port we listed in the DESTINATION line in carbon.conf on the carbon-relay server</li>
</ul>


<p>The other key is configuring the local_settings.py config file for the webapp correctly:</p>

<div>
  <pre><code class='properties'>CLUSTER_SERVERS = [ 'cache1','cache2' ]
MEMCACHE_HOSTS = [ 'memcache1.graphite:11211' ]</code></pre>
</div>


<p>The only mandatory piece of information in local_settings.py is CLUSTER_SERVERS, which must be a Python array containing strings that are the names (or ip address) of every carbon-cache server. You can optionally include an array of memcached instances to improve web performance dramatically. If you choose to utilize memcached, you can either run it locally on each carbon-cache instance or you can run a cluster of one or more memcached servers remotely. In the example above, we&#8217;re using a single memcached instance running on a remote server.</p>

<p>IMPORTANT NOTE: The local_settings.py file MUST be readable by the web server user. If you&#8217;re running Apache, this is typically the apache user. If the local_settings.py file is NOT readable, you will get very very strange errors.</p>

<h2>Firewall Rules</h2>

<p>To get this all working, specific ports must be open on the various firewalls. If you&#8217;re using Amazon EC2, your security groups will need to contain rules explicitly allowing incoming traffic on these ports:</p>

<h3>carbon-relay</h3>

<ol>
<li>Incoming on port 2003</li>
</ol>


<h3>carbon-cache/webapp</h3>

<ol>
<li>Incoming on port 2004 (messages from carbon-relay)</li>
<li>Incoming on port 2003 (requests for data from other carbon-cache servers)</li>
<li>Incoming on port 80 (web requests)</li>
<li>Incoming on port 11211 (if you&#8217;re running memcached locally)</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Logstash grok filters]]></title>
    <link href="http://organicveggie.github.com/blog/2012/05/15/testing-logstash-grok-filters/"/>
    <updated>2012-05-15T13:25:00-07:00</updated>
    <id>http://organicveggie.github.com/blog/2012/05/15/testing-logstash-grok-filters</id>
    <content type="html"><![CDATA[<p>Logstash is an outstanding tool for collecting and parsing logfiles. In particular, the grok filter is extremely useful to extract specific pieces of data from your logfiles. Once you pull data out of the logfiles into fields, you can easily search on those fields. Unfortunately, I find the format for patterns in grok filter challenging to write correctly. If you mess up your pattern, you will end up with the dreaded &#8220;_grokparsefailure&#8221; tag on your log entry. What you need is a way to test your patterns before adding them to Logstash.</p>

<!-- more -->


<p>As it turns out, you can test your patterns using the grok ruby library. @a1cy demonstrated the basics to me, but I wanted to describe the full process.</p>

<h2>Requirements</h2>

<ul>
<li>Ruby 1.9+</li>
<li>jls-grok gem</li>
</ul>


<p>On my machine, I chose to install Ruby 1.9.2 via RVM, but you can use whatever technique you want. If you try to use Ruby 1.8.7, you will get a syntax error when you load the grok gem.</p>

<h2>Load Grok</h2>

<p>You want to use the &#8220;grok-pure&#8221; library, which is 100% ruby, rather than &#8220;grok&#8221;, which depends on a C library.</p>

<div>
  <pre><code class='ruby'>require 'rubygems'
require 'grok-pure'
grok = Grok.new</code></pre>
</div>


<h2>Loading Patterns</h2>

<p>Since the grok filter in Logstash depends heavily on pattern files, I recommend you download the standard patterns from Github. Either the standard set for Logstash:</p>

<p><a href="https://github.com/logstash/logstash/tree/master/patterns">https://github.com/logstash/logstash/tree/master/patterns</a></p>

<p>Or you can use the ones with grok:</p>

<p><a href="https://github.com/jordansissel/ruby-grok/blob/master/patterns/pure-ruby">https://github.com/jordansissel/ruby-grok/blob/master/patterns/pure-ruby</a></p>

<p>Dump these into a directory so that you can load them with grok.</p>

<div>
  <pre><code class='ruby'>grok.add_patterns_from_file(&quot;/path/to/patterns/grok-patterns&quot;)</code></pre>
</div>


<p>You can use the same technique to load your own pattern files. Or you can manually add patterns by name:</p>

<div>
  <pre><code class='ruby'>grok.add_pattern(&quot;foo&quot;, &quot;.*foo.*&quot;)
grok.add_pattern(&quot;bar&quot;, &quot;.*bar.*&quot;)</code></pre>
</div>


<h2>Testing Patterns</h2>

<p>So, you have successfully loaded a set of patterns. How do you test something? Let&#8217;s take a sample log line from a MongoDB logfile:</p>

<p><code>Tue May 15 11:21:42 [conn1047685] moveChunk deleted: 7157</code></p>

<p>Let&#8217;s say we want to grab the date/time as well as the number of the deleted chunk. Unfortunately, the date/time format doesn&#8217;t match any of the stock patterns. We can build up a new date pattern and test it each step of the way:</p>

<div>
  <pre><code class='ruby'>text = &quot;Tue May 15 11:21:42 [conn1047685] moveChunk deleted: 7157&quot;
pattern = '%{DAY}'
grok.compile(pattern)
grok.match(text)</code></pre>
</div>


<p>The last line returns false if the pattern fails to match the input text. If the pattern matches the input text, Grok.match() returns a Grok::Match object that contains a lot of useful information about the match. You can ask Grok to output the text it capture by using the captures() method:</p>

<div>
  <pre><code class='ruby'>grok.match(text).captures()
 =&gt; {&quot;DAY&quot;=&gt;[&quot;Tue&quot;]}</code></pre>
</div>


<p>You can also test named captures, the same way you would use them in the Logstash grok filter:</p>

<div>
  <pre><code class='ruby'>pattern = '%{DAY:day_of_week}'
grok.compile(pattern)
grok.match(text).captures()
 =&gt; {&quot;DAY:day_of_week&quot;=&gt;[&quot;Tue&quot;]}</code></pre>
</div>


<p>The full date/time pattern for this example looks like the following:</p>

<div>
  <pre><code class='ruby'>pattern = '%{DAY} %{MONTH} %{MONTHDAY} %{TIME}'
grok.compile(pattern)
grok.match(text).capture()
 =&gt; {&quot;DAY&quot;=&gt;[&quot;Tue&quot;], &quot;MONTH&quot;=&gt;[&quot;May&quot;], &quot;MONTHDAY&quot;=&gt;[&quot;15&quot;], &quot;TIME&quot;=&gt;[&quot;11:21:42&quot;], &quot;HOUR&quot;=&gt;[&quot;11&quot;], &quot;MINUTE&quot;=&gt;[&quot;21&quot;], &quot;SECOND&quot;=&gt;[&quot;42&quot;]}</code></pre>
</div>


<p>Now let&#8217;s say we want to re-use this date/time pattern and still extract the chunk id from the log line. We can add a named pattern to the grok object manually and then reuse it, the same way we used patterns from the files:</p>

<div>
  <pre><code class='ruby'>grok.add_pattern(&quot;MONGO_TIMESTAMP&quot;, '%{DAY} %{MONTH} %{MONTHDAY} %{TIME}')
grok.compile(&quot;%{MONGO_TIMESTAMP}&quot;)
pattern = '%{MONGO_TIMESTAMP:timestamp}'
grok.compile(pattern)
grok.match(text).captures()
 =&gt; {&quot;MONGO_TIMESTAMP:timestamp&quot;=&gt;[&quot;Tue May 15 11:21:42&quot;], &quot;DAY&quot;=&gt;[&quot;Tue&quot;], &quot;MONTH&quot;=&gt;[&quot;May&quot;], &quot;MONTHDAY&quot;=&gt;[&quot;15&quot;], &quot;TIME&quot;=&gt;[&quot;11:21:42&quot;], &quot;HOUR&quot;=&gt;[&quot;11&quot;], &quot;MINUTE&quot;=&gt;[&quot;21&quot;], &quot;SECOND&quot;=&gt;[&quot;42&quot;]}</code></pre>
</div>


<p>We still want the chunk id, so we can use additional patterns:</p>

<div>
  <pre><code class='ruby'>pattern = '%{MONGO_TIMESTAMP:timestamp}%{GREEDYDATA} moveChunk deleted: %{NUMBER:chunk_id}'
grok.compile(pattern)
grok.match(text).captures()
 =&gt; {&quot;MONGO_TIMESTAMP:timestamp&quot;=&gt;[&quot;Tue May 15 11:21:42&quot;], &quot;DAY&quot;=&gt;[&quot;Tue&quot;], &quot;MONTH&quot;=&gt;[&quot;May&quot;], &quot;MONTHDAY&quot;=&gt;[&quot;15&quot;], &quot;TIME&quot;=&gt;[&quot;11:21:42&quot;], &quot;HOUR&quot;=&gt;[&quot;11&quot;], &quot;MINUTE&quot;=&gt;[&quot;21&quot;], &quot;SECOND&quot;=&gt;[&quot;42&quot;], &quot;GREEDYDATA&quot;=&gt;[&quot; [conn1047685]&quot;], &quot;NUMBER:chunk_id&quot;=&gt;[&quot;7157&quot;], &quot;BASE10NUM&quot;=&gt;[&quot;7157&quot;]}</code></pre>
</div>


<p>Let&#8217;s say we wanted to grab that connection number inside the square brackets:</p>

<div>
  <pre><code class='ruby'>pattern = '%{MONGO_TIMESTAMP:timestamp} \\[conn%{NUMBER:connection}\\] moveChunk deleted: %{NUMBER:chunk_id}'
grok.compile(pattern)
grok.match(text).captures()
 =&gt; {&quot;MONGO_TIMESTAMP:timestamp&quot;=&gt;[&quot;Tue May 15 11:21:42&quot;], &quot;DAY&quot;=&gt;[&quot;Tue&quot;], &quot;MONTH&quot;=&gt;[&quot;May&quot;], &quot;MONTHDAY&quot;=&gt;[&quot;15&quot;], &quot;TIME&quot;=&gt;[&quot;11:21:42&quot;], &quot;HOUR&quot;=&gt;[&quot;11&quot;], &quot;MINUTE&quot;=&gt;[&quot;21&quot;], &quot;SECOND&quot;=&gt;[&quot;42&quot;], &quot;NUMBER:connection&quot;=&gt;[&quot;1047685&quot;], &quot;BASE10NUM&quot;=&gt;[&quot;1047685&quot;, &quot;7157&quot;], &quot;NUMBER:chunk_id&quot;=&gt;[&quot;7157&quot;]}</code></pre>
</div>


<h2>Full Example</h2>

<div>
  <pre><code class='ruby'>require 'rubygems'
require 'grok-pure'
 
grok = Grok.new
 
text = &quot;Tue May 15 11:21:42 [conn1047685] moveChunk deleted: 7157&quot;
pattern = '%{MONGO_TIMESTAMP:timestamp} \\[conn%{NUMBER:connection}\\] moveChunk deleted: %{NUMBER:chunk_id}'
grok.compile(pattern)
grok.match(text).captures()
 =&gt; {&quot;MONGO_TIMESTAMP:timestamp&quot;=&gt;[&quot;Tue May 15 11:21:42&quot;], &quot;DAY&quot;=&gt;[&quot;Tue&quot;], &quot;MONTH&quot;=&gt;[&quot;May&quot;], &quot;MONTHDAY&quot;=&gt;[&quot;15&quot;], &quot;TIME&quot;=&gt;[&quot;11:21:42&quot;], &quot;HOUR&quot;=&gt;[&quot;11&quot;], &quot;MINUTE&quot;=&gt;[&quot;21&quot;], &quot;SECOND&quot;=&gt;[&quot;42&quot;], &quot;NUMBER:connection&quot;=&gt;[&quot;1047685&quot;], &quot;BASE10NUM&quot;=&gt;[&quot;1047685&quot;, &quot;7157&quot;], &quot;NUMBER:chunk_id&quot;=&gt;[&quot;7157&quot;]}</code></pre>
</div>


<p>You can also find an ultra simple example in the grok example folder on Github: <a href="https://github.com/jordansissel/ruby-grok/blob/master/examples/test.rb">https://github.com/jordansissel/ruby-grok/blob/master/examples/test.rb</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why PostgreSQL?]]></title>
    <link href="http://organicveggie.github.com/blog/2010/10/14/why-postgresql/"/>
    <updated>2010-10-14T22:49:00-07:00</updated>
    <id>http://organicveggie.github.com/blog/2010/10/14/why-postgresql</id>
    <content type="html"><![CDATA[<p>Quick overview of what PostgreSQL brings to the table that is not available in MySQL.</p>

<!-- more -->


<ul>
<li>Uses MVCC for all tables providing:

<ul>
<li>Fully transactional including ACID compliance for consistency</li>
<li>Nested transactions</li>
</ul>
</li>
<li>SQL 2008 compliant</li>
<li>Foreign keys for any table</li>
<li>Advanced table partitioning</li>
<li>Highly sophisticated query planner/optimizer

<ul>
<li>Can split up a query for execution across multiple CPUs simultaneously</li>
<li>Collects internal statistics for adaptive query planning</li>
<li>Special genetic query optimizer for queries with large numbers of joins</li>
<li>Supports multiple indexes per table per query</li>
</ul>
</li>
<li>Advanced support for query &amp; results caching</li>
<li>Hot/online backup</li>
<li>Point-in-time-recovery</li>
<li>Write-ahead logs for fault-tolerance</li>
<li>Tablespaces for controlling physical disk layout</li>
<li>Native asynchronous replication guaranteeing identical results on all machines. Supports both:

<ul>
<li>Streaming replication</li>
<li>Hot standby</li>
</ul>
</li>
<li>Partial indexes</li>
<li>Index creation/removal does not lock table</li>
<li>Full support for constraints</li>
<li>Transactional DDL - changes like table modifications can placed inside a transaction and rolled back</li>
</ul>


<p>Specific disadvantages to MySQL:</p>

<ul>
<li>Confusion with table types - MyISAM vs InnoDB</li>
<li>Designed to scale out not up - does not utilize larger numbers of cores efficiently and cannot spread queries across cores</li>
<li>Hot backup of is difficult for databases containing both InnoDB and MyISAM</li>
<li>Replication is mediocre and error prone</li>
<li>InnoDB stores the data with the primary key, so any queries using secondary indices are slower</li>
<li>Subqueries not well optimized</li>
<li>Only uses a single index per table per query</li>
<li>Index creation/removal requires an exclusive write lock</li>
<li>MyISAM only offers table level locking which causes severe performance degradation under heavy concurrency</li>
<li>Limited support for constraints</li>
<li>No transactional DDL - changes like table modifications are automatically committed and cannot be rolled back</li>
</ul>


<p>MySQL offers the following advantages over PostgreSQL:</p>

<ul>
<li>MyISAM tables can offer better read performance, specifically for simple SELECT queries, but at the cost of no support for transactions, foreign keys or data guarantees</li>
<li>COUNT(*) on MyISAM is very fast and slow on PostgreSQL</li>
<li>INSERT IGNORE and INSERT&#8230;ON DUPLICATE UPDATE</li>
</ul>

]]></content>
  </entry>
  
</feed>
